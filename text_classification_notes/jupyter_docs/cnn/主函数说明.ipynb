{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "import data_helpers\n",
    "from model import TextModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据处理的时候需要padding\n",
    "Pad each sentence to the maximum sentence length, which turns out to be 59. We append special <PAD> tokens to all other sentences to make them 59 words. Padding sentences to the same length is useful because it allows us to efficiently batch our data since each example in a batch must be of the same length.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过tf.flags.DEFINE_xxx定义一些默认参数，并在适当的时候通过命令行参数改写默认值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Data loading params\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "# eval parameters\n",
    "tf.flags.DEFINE_string(\"checkpoint_dir\", \"\", \"Checkpoint directory from training run\")\n",
    "tf.flags.DEFINE_boolean(\"eval_train\", False, \"Evaluate on all training data\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练过程如下：  \n",
    "从模型中获取必要的变量，后续就可以进行训练/评估/以及记录到summary了：  \n",
    "prediction, loss, optimize, accuracy = cnn_text.get_model_variables()  \n",
    "在所有变量定义好之后，再调用global_variables_initializer对变量进行初始化  \n",
    "            # initialize all variables  \n",
    "            init_g = tf.global_variables_initializer()  \n",
    "            init_l = tf.local_variables_initializer()  \n",
    "            sess.run(init_l)  \n",
    "            sess.run(init_g)  \n",
    "训练的时候只要设置好输入的feed_dict以及调用训练步骤：  \n",
    "            def train_step(x_batch, y_batch):  \n",
    "                \"\"\"  \n",
    "                train_step  \n",
    "                \"\"\"  \n",
    "                feed_dict = {  \n",
    "                    cnn_text.data: x_batch,  \n",
    "                    cnn_text.target: y_batch,  \n",
    "                    cnn_text.dropout_keep_prob: FLAGS.dropout_keep_prob  \n",
    "                }  \n",
    "                _, summaries, train_loss, train_accuracy = sess.run(  \n",
    "                            [optimize, train_summary_op, loss, accuracy],  \n",
    "                            feed_dict)  \n",
    "                time_str = datetime.datetime.now().isoformat()  \n",
    "                current_step = tf.train.global_step(sess, cnn_text.global_step)  \n",
    "                print(\"{0}: step {1},  loss {2:g},  acc {3:g}\".format(time_str, current_step, train_loss, train_accuracy))  \n",
    "                train_summary_writer.add_summary(summaries)  \n",
    "注意，这里的step定义是以一个小批量一个小批量递增的，而不是整个数据集合的大小来递增的：  \n",
    "                current_step = tf.train.global_step(sess, cnn_text.global_step)  \n",
    "eval的过程与训练过程类似，只是两者run的op不一样罢了：  \n",
    "                _, summaries, train_loss, train_accuracy = sess.run(\n",
    "                            [optimize, train_summary_op, loss, accuracy],\n",
    "                            feed_dict)  \n",
    "\n",
    "最后的训练过程就是标准的对数据批量进行便利的过程：  \n",
    "            for batch in batches:  \n",
    "                x_batch, y_batch = zip(*batch)  \n",
    "                train_step(x_batch, y_batch)  \n",
    "                current_step = tf.train.global_step(sess, cnn_text.global_step)  \n",
    "                if 0 == current_step % FLAGS.evaluate_every:  \n",
    "                    eval_step(x_dev, y_dev)  \n",
    "                if 0 == current_step % FLAGS.checkpoint_every:  \n",
    "                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)  \n",
    "                    print(\"saved model checkpoint to {0}\".format(path))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The allow_soft_placement setting allows TensorFlow to fall back on a device with a certain operation implemented when the preferred device doesn’t exist. For example, if our code places an operation on a GPU and we run the code on a machine without GPU, not using allow_soft_placement would result in an error. If log_device_placement is set, TensorFlow log on which devices (CPU or GPU) it places operations. That’s useful for debugging. FLAGS are command-line arguments to our program.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # parse arguments\n",
    "    FLAGS(sys.argv)\n",
    "    print(FLAGS.batch_size)\n",
    "\n",
    "    # This is not working any more, because api has been changed!!!\n",
    "    print(\"\\nParameters:\")\n",
    "    for attr, value in sorted(FLAGS.__flags.items()):\n",
    "        print(\"{}={}\".format(attr.upper(), value))\n",
    "    print(\"\")\n",
    "\n",
    "    # Data Preparation\n",
    "    # ==================================================\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    x_text, y = data_helpers.load_data_and_labels(FLAGS.positive_data_file, FLAGS.negative_data_file)\n",
    "    \n",
    "    # Build vocabulary\n",
    "    max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "    x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "    \n",
    "    # Randomly shuffle data\n",
    "    np.random.seed(10)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "    x_shuffled = x[shuffle_indices]\n",
    "    y_shuffled = y[shuffle_indices]\n",
    "    \n",
    "    # Split train/test set\n",
    "    # TODO: This is very crude, should use cross-validation\n",
    "    dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
    "    x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "    y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "    \n",
    "    del x, y, x_shuffled, y_shuffled\n",
    "    \n",
    "    print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "    print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "\n",
    "    # Training\n",
    "    # ==================================================\n",
    "    with tf.Graph().as_default():\n",
    "        sequence_length = x_train.shape[1]\n",
    "        num_classes = y_train.shape[1]\n",
    "        input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        session_conf = tf.ConfigProto(\n",
    "                        allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "                        log_device_placement=FLAGS.log_device_placement)\n",
    "        with tf.Session(config=session_conf) as sess:\n",
    "            cnn_text = TextModel(\n",
    "                            input_x, input_y,\n",
    "                            max_sequence_len=sequence_length,\n",
    "                            num_classes=num_classes,\n",
    "                            vocab_size=len(vocab_processor.vocabulary_),\n",
    "                            embedding_size=FLAGS.embedding_dim,\n",
    "                            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "                            num_filters=FLAGS.num_filters,\n",
    "                            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "            prediction, loss, optimize, accuracy = cnn_text.get_model_variables()\n",
    "\n",
    "            # Output directory for models and summaries\n",
    "            timestamp = str(int(time.time()))\n",
    "            out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "            print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "            # summaries for loss and accuracy\n",
    "            loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "            acc_summary = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "            # train summaries\n",
    "            train_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "            train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "            # eval summaries\n",
    "            eval_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "            eval_summary_dir = os.path.join(out_dir, \"summaries\", \"eval\")\n",
    "            eval_summary_writer = tf.summary.FileWriter(eval_summary_dir, sess.graph)\n",
    "\n",
    "            # checkpoint directory\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "            # tensorflow assumes this directory already exists, so we need to create it if it not exists\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "            # write vocabulary\n",
    "            vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "            # initialize all variables\n",
    "            init_g = tf.global_variables_initializer()\n",
    "            init_l = tf.local_variables_initializer()\n",
    "            sess.run(init_l)\n",
    "            sess.run(init_g)\n",
    "\n",
    "            def train_step(x_batch, y_batch):\n",
    "                \"\"\"\n",
    "                train_step\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                    cnn_text.data: x_batch,\n",
    "                    cnn_text.target: y_batch,\n",
    "                    cnn_text.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "                }\n",
    "                _, summaries, train_loss, train_accuracy = sess.run(\n",
    "                            [optimize, train_summary_op, loss, accuracy],\n",
    "                            feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                current_step = tf.train.global_step(sess, cnn_text.global_step)\n",
    "                print(\"{0}: step {1},  loss {2:g},  acc {3:g}\".format(time_str, current_step, train_loss, train_accuracy))\n",
    "                train_summary_writer.add_summary(summaries)\n",
    "\n",
    "            def eval_step(x_batch, y_batch):\n",
    "                \"\"\"\n",
    "                eval_step\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                    cnn_text.data: x_batch,\n",
    "                    cnn_text.target: y_batch,\n",
    "                    cnn_text.dropout_keep_prob: 1.0\n",
    "                }\n",
    "                step, summaries, eval_loss, eval_accuracy = sess.run(\n",
    "                            [cnn_text.global_step, eval_summary_op, loss, accuracy],\n",
    "                            feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"evaluation {0}: step {1},  loss {2:g},  acc {3:g}\".format(time_str, step, eval_loss, eval_accuracy))\n",
    "                eval_summary_writer.add_summary(summaries)\n",
    "                \n",
    "            \n",
    "            # generate batches\n",
    "            batches = data_helpers.batch_iter(\n",
    "                        list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "\n",
    "            # training loop, for each batch ...\n",
    "            for batch in batches:\n",
    "                x_batch, y_batch = zip(*batch)\n",
    "                train_step(x_batch, y_batch)\n",
    "                current_step = tf.train.global_step(sess, cnn_text.global_step)\n",
    "                if 0 == current_step % FLAGS.evaluate_every:\n",
    "                    eval_step(x_dev, y_dev)\n",
    "                if 0 == current_step % FLAGS.checkpoint_every:\n",
    "                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                    print(\"saved model checkpoint to {0}\".format(path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By defining a global_step variable and passing it to the optimizer we allow TensorFlow handle the counting of training steps for us. The global step will be automatically incremented by one every time you execute train_op.  \n",
    "\n",
    "train_op here is a newly created operation that we can run to perform a gradient update on our parameters. Each execution of train_op is a training step. TensorFlow automatically figures out which variables are “trainable” and calculates their gradients.  \n",
    "\n",
    "TensorFlow has a concept of a summaries, which allow you to keep track of and visualize various quantities during training and evaluation. For example, you probably want to keep track of how your loss and accuracy evolve over time. You can also keep track of more complex quantities, such as histograms of layer activations. Summaries are serialized objects, and they are written to disk using a SummaryWriter.  \n",
    "\n",
    "Another TensorFlow feature you typically want to use is checkpointing – saving the parameters of your model to restore them later on. Checkpoints can be used to continue training at a later point, or to pick the best parameters setting using early stopping. Checkpoints are created using a Saver object.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测过程：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate():\n",
    "    # parse arguments\n",
    "    FLAGS(sys.argv)\n",
    "    print(FLAGS.batch_size)\n",
    "    \n",
    "    # map data into vocabulary\n",
    "    vocab_path = os.path.join(FLAGS.checkpoint_dir, \"..\", \"vocab\")\n",
    "    print(vocab_path)\n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor.restore(vocab_path)\n",
    "\n",
    "    # CHANGE THIS: Load data. Load your own data here\n",
    "    if FLAGS.eval_train:\n",
    "        x_raw, y_test = data_helpers.load_data_and_labels(FLAGS.positive_data_file, FLAGS.negative_data_file)\n",
    "        y_test = np.argmax(y_test, axis=1)\n",
    "    else:\n",
    "        x_raw = [\"a masterpiece four years in the making\", \"everything is off.\"]\n",
    "        y_test = [1, 0] \n",
    "\n",
    "    x_test = np.array(list(vocab_processor.transform(x_raw)))\n",
    "    print(\"\\nEvaluating...\\n\")\n",
    "    \n",
    "    # Evaluation\n",
    "    # ==================================================\n",
    "    checkpoint_file = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "                            allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "                            log_device_placement=FLAGS.log_device_placement)\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            # Load the saved meta graph and restore variables\n",
    "            saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "            saver.restore(sess, checkpoint_file)\n",
    "\n",
    "            # Get the placeholders from the graph by name\n",
    "            input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
    "            # input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n",
    "            dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
    "\n",
    "            # Tensors we want to evaluate\n",
    "            predictions = graph.get_operation_by_name(\"cnn_output/predictions\").outputs[0]\n",
    "\n",
    "            # Generate batches for one epoch\n",
    "            batches = data_helpers.batch_iter(list(x_test), FLAGS.batch_size, 1, shuffle=False)\n",
    "\n",
    "            # Collect the predictions here\n",
    "            all_predictions = []\n",
    "\n",
    "            for x_test_batch in batches:\n",
    "                batch_predictions = sess.run(predictions, {input_x: x_test_batch, dropout_keep_prob: 1.0})\n",
    "                all_predictions = np.concatenate([all_predictions, batch_predictions])\n",
    "\n",
    "    # Print accuracy if y_test is defined\n",
    "    if y_test is not None:\n",
    "        correct_predictions = float(sum(all_predictions == y_test))\n",
    "        print(\"Total number of test examples: {}\".format(len(y_test)))\n",
    "        print(\"Accuracy: {:g}\".format(correct_predictions/float(len(y_test))))\n",
    "\n",
    "    # Save the evaluation to a csv\n",
    "    predictions_human_readable = np.column_stack((np.array(x_raw), all_predictions))\n",
    "    out_path = os.path.join(FLAGS.checkpoint_dir, \"..\", \"prediction.csv\")\n",
    "    print(\"Saving evaluation to {0}\".format(out_path))\n",
    "    with open(out_path, 'w') as f:\n",
    "        import csv\n",
    "        csv.writer(f).writerows(predictions_human_readable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(task=None):\n",
    "    if \"train\" == task:\n",
    "        train()\n",
    "    if \"eval\" == task:\n",
    "        # python basic.py --checkpoint_dir=/data/aitc/zs/study/fin_message/basic/runs/1525335105/checkpoints/\n",
    "        evaluate()\n",
    "    if task is None:\n",
    "        if not FLAGS.eval_train:\n",
    "            train()\n",
    "        else:\n",
    "            evaluate()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
